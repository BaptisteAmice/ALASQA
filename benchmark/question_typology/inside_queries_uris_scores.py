# Metrics based on URI extraction from SPARQL queries
# THe purpose isn't to have them at 100% but to have a good indicator of the quality of the system
# Can particularly be used to test queries generated by LLMs to see if they tend to hallucinate URIs
import json
import re

def extract_final_parts(query):
    """
    Extract final parts of full URIs, including prefixes like wd:Q928726.
    """
    final_parts = set()

    # 1) Match <...> URIs
    full_uri_pattern = re.compile(r'<([^>]+)>')
    for match in full_uri_pattern.finditer(query):
        uri = match.group(1)
        last_delim = max(uri.rfind('/'), uri.rfind('#'), uri.rfind(':'))
        if last_delim != -1:
            final_parts.add(uri[last_delim + 1:])

    # 2) Match prefixed forms like wd:Q928726
    prefix_uri_pattern = re.compile(r'(\w+):([A-Za-z0-9_]+)')
    for match in prefix_uri_pattern.finditer(query):
        prefix, local = match.groups()
        # Ignore prefixes like SELECT, WHERE, etc.
        if prefix not in {"SELECT", "WHERE", "LIMIT", "ORDER", "FILTER"}:
            final_parts.add(local)

    return final_parts

def compute_metrics(gold, system):
    true_pos = gold & system
    if not system:
        precision = 1.0 if not gold else 0.0
    else:
        precision = len(true_pos) / len(system)

    recall = len(true_pos) / len(gold) if gold else 0.0

    if precision + recall == 0:
        f1 = 0.0
    else:
        f1 = 2 * precision * recall / (precision + recall)
    return precision, recall, f1

if __name__ == "__main__":
    file = r'C:\Users\PC\Desktop\llmSparklis\benchmark\BestOutputs\for_paper\QALD9Plus\dbpedia\test\one_shot_the_most\best_suggestion\QALD-9-plus_sparklisllm-LLMFrameworkOneShotTheMost_20250604_111013.json'
    # Load JSON input from a file or a string
    with open(file, "r") as f:
        data = json.load(f)

    precisions = []
    recalls = []
    f1_scores = []

    # Loop through the questions
    for qid, qdata in data["Data"].items():
        b_query = qdata.get("BenchmarkQuery", "")
        s_query = qdata.get("SystemQuery", "")

        gold_uris = extract_final_parts(b_query)
        sys_uris = extract_final_parts(s_query)

        precision, recall, f1 = compute_metrics(gold_uris, sys_uris)

        print(f"Question {qid}:")
        print(f"  Benchmark URIs: {gold_uris}")
        print(f"  System URIs: {sys_uris}")
        print(f"  Precision: {precision:.2f}")
        print(f"  Recall: {recall:.2f}")
        print(f"  F1-score: {f1:.2f}\n")

        precisions.append(precision)
        recalls.append(recall)
        f1_scores.append(f1)

    # Calculate overall metrics
    overall_precision = sum(precisions) / len(precisions) if precisions else 0.0
    overall_recall = sum(recalls) / len(recalls) if recalls else 0.0
    overall_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0
    print("Overall Metrics:")
    print(f"  Overall Precision: {overall_precision:.2f}")
    print(f"  Overall Recall: {overall_recall:.2f}")
    print(f"  Overall F1-score: {overall_f1:.2f}")

