"""
Metrics based on extracting URIs from SPARQL queries.

The goal is not to reach 100% for these metrics but to provide a useful indicator 
of how well the system generates correct URIs.

This can be especially helpful for evaluating SPARQL queries generated by LLMs,
to check whether they hallucinate or misuse URIs.
"""
import json
import re

def extract_final_parts(query):
    """
    Extract final parts of full URIs, excluding namespace declarations.
    """
    final_parts = set()

    # 1) Match <...> URIs that are NOT in PREFIX declarations
    full_uri_pattern = re.compile(r'PREFIX\s+\w+:\s*<([^>]+)>|<([^>]+)>')

    for match in full_uri_pattern.finditer(query):
        prefix_uri = match.group(1)
        normal_uri = match.group(2)

        # Only keep normal URIs in triples, not prefixes
        if normal_uri:
            uri = normal_uri
            last_delim = max(uri.rfind('/'), uri.rfind('#'), uri.rfind(':'))
            if last_delim != -1 and last_delim + 1 < len(uri):
                final_parts.add(uri[last_delim + 1:])

    # 2) Match prefixed forms like wd:Q928726
    prefix_uri_pattern = re.compile(r'(\w+):([A-Za-z0-9_]+)')
    for match in prefix_uri_pattern.finditer(query):
        prefix, local = match.groups()
        if prefix not in {"PREFIX", "SELECT", "WHERE", "LIMIT", "ORDER", "FILTER", "ASK"}:
            final_parts.add(local)

    return final_parts

def compute_metrics(gold, system):
    true_pos = gold & system
    if not system:
        precision = 1.0 if not gold else 0.0
    else:
        precision = len(true_pos) / len(system)

    recall = len(true_pos) / len(gold) if gold else 0.0

    if precision + recall == 0:
        f1 = 0.0
    else:
        f1 = 2 * precision * recall / (precision + recall)
    return precision, recall, f1

if __name__ == "__main__":
    file = r'C:\Users\PC\Desktop\llmSparklis\QALD-9-plus_sparklisllm-LLMFrameworkSimpleBooleans_20250627_071715.json'
    # Load JSON input from a file or a string
    with open(file, "r") as f:
        data = json.load(f)

    precisions = []
    recalls = []
    f1_scores = []

    # Loop through the questions
    for qid, qdata in data["Data"].items():
        b_query = qdata.get("BenchmarkQuery", "")
        s_query = qdata.get("SystemQuery", "")

        gold_uris = extract_final_parts(b_query)
        sys_uris = extract_final_parts(s_query)

        precision, recall, f1 = compute_metrics(gold_uris, sys_uris)

        print(f"Question {qid}:")
        print(f"  Benchmark URIs: {gold_uris}")
        print(f"  System URIs: {sys_uris}")
        print(f"  Precision: {precision:.2f}")
        print(f"  Recall: {recall:.2f}")
        print(f"  F1-score: {f1:.2f}\n")

        precisions.append(precision)
        recalls.append(recall)
        f1_scores.append(f1)

    # Calculate overall metrics
    overall_precision = sum(precisions) / len(precisions) if precisions else 0.0
    overall_recall = sum(recalls) / len(recalls) if recalls else 0.0
    overall_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0
    print("Overall Metrics:")
    print(f"  Overall Precision: {overall_precision:.2f}")
    print(f"  Overall Recall: {overall_recall:.2f}")
    print(f"  Overall F1-score: {overall_f1:.2f}")

