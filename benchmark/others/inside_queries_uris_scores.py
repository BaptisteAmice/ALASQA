"""
Metrics based on extracting URIs from SPARQL queries.

The goal is not to reach 100% for these metrics but to provide a useful indicator 
of how well the system generates correct URIs.

This can be especially helpful for evaluating SPARQL queries generated by LLMs,
to check whether they hallucinate or misuse URIs.
"""
import json
import re
import statistics

def extract_final_parts(query):
    """
    Extract final parts of full URIs, excluding namespace declarations.
    """
    final_parts = set()

    # 1) Match <...> URIs that are NOT in PREFIX declarations
    full_uri_pattern = re.compile(r'PREFIX\s+\w+:\s*<([^>]+)>|<([^>]+)>')

    for match in full_uri_pattern.finditer(query):
        prefix_uri = match.group(1)
        normal_uri = match.group(2)

        # Only keep normal URIs in triples, not prefixes
        if normal_uri:
            uri = normal_uri
            last_delim = max(uri.rfind('/'), uri.rfind('#'), uri.rfind(':'))
            if last_delim != -1 and last_delim + 1 < len(uri):
                final_parts.add(uri[last_delim + 1:])

    # 2) Match prefixed forms like wd:Q928726
    prefix_uri_pattern = re.compile(r'(\w+):([A-Za-z0-9_]+)')
    for match in prefix_uri_pattern.finditer(query):
        prefix, local = match.groups()
        if prefix not in {"PREFIX", "SELECT", "WHERE", "LIMIT", "ORDER", "FILTER", "ASK"}:
            final_parts.add(local)

    return final_parts

def compute_metrics(gold, system):
    true_pos = gold & system
    if not system:
        precision = 1.0 if not gold else 0.0
    else:
        precision = len(true_pos) / len(system)

    recall = len(true_pos) / len(gold) if gold else 0.0

    if precision + recall == 0:
        f1 = 0.0
    else:
        f1 = 2 * precision * recall / (precision + recall)
    return precision, recall, f1

if __name__ == "__main__":
    input_files = [
        r'C:\Users\PC\Desktop\llmSparklis\benchmark\BestOutputs\for_egc\QALD9Plus\Wikidata\train\SimpleBoolean\greedy\QALD-9-plus_sparklisllm-LLMFrameworkBooleanByMergeByPatterns_20250707_213735.json',
        r'C:\Users\PC\Desktop\llmSparklis\benchmark\BestOutputs\for_egc\QALD9Plus\Wikidata\train\SimpleBoolean\greedy\QALD-9-plus_sparklisllm-LLMFrameworkBooleanByMergeByPatterns_20250708_020708.json',
    ]

    file_precisions = []
    file_recalls = []
    file_f1s = []

    for file in input_files:
        precisions = []
        recalls = []
        f1_scores = []

        with open(file, "r") as f:
            data = json.load(f)

        for qid, qdata in data["Data"].items():
            b_query = qdata.get("BenchmarkQuery", "")
            s_query = qdata.get("SystemQuery", "")

            gold_uris = extract_final_parts(b_query)
            sys_uris = extract_final_parts(s_query)

            precision, recall, f1 = compute_metrics(gold_uris, sys_uris)

            print(f"File: {file} | Question {qid}:")
            print(f"  Benchmark URIs: {gold_uris}")
            print(f"  System URIs: {sys_uris}")
            print(f"  Precision: {precision:.2f}")
            print(f"  Recall: {recall:.2f}")
            print(f"  F1-score: {f1:.2f}\n")

            precisions.append(precision)
            recalls.append(recall)
            f1_scores.append(f1)

        file_precision = sum(precisions) / len(precisions) if precisions else 0.0
        file_recall = sum(recalls) / len(recalls) if recalls else 0.0
        file_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0

        file_precisions.append(file_precision)
        file_recalls.append(file_recall)
        file_f1s.append(file_f1)

        print(f"File Summary: {file}")
        print(f"  Mean Precision: {file_precision:.2f}")
        print(f"  Mean Recall: {file_recall:.2f}")
        print(f"  Mean F1-score: {file_f1:.2f}\n")

    mean_precision = statistics.mean(file_precisions)
    mean_recall = statistics.mean(file_recalls)
    mean_f1 = statistics.mean(file_f1s)

    std_precision = statistics.stdev(file_precisions) if len(file_precisions) > 1 else 0.0
    std_recall = statistics.stdev(file_recalls) if len(file_recalls) > 1 else 0.0
    std_f1 = statistics.stdev(file_f1s) if len(file_f1s) > 1 else 0.0

    print("=== Overall Metrics ===")
    print(f"  Precision: {mean_precision:.2f} ± {std_precision:.2f}")
    print(f"  Recall:    {mean_recall:.2f} ± {std_recall:.2f}")
    print(f"  F1-score:  {mean_f1:.2f} ± {std_f1:.2f}")